# ðŸš€ T1D PINN MLOps Deployment Guide

**Complete production-grade MLOps stack inspired by J0MT/AI_Drug_Discovery**

---

## ðŸ“¦ **What I've Created For You**

### âœ… **Core Files (Ready to Use)**

1. **[PROJECT_STRUCTURE.md](computer:///mnt/user-data/outputs/PROJECT_STRUCTURE.md)** - Complete project organization
2. **[Dockerfile](computer:///mnt/user-data/outputs/Dockerfile)** - Production container image
3. **[docker-compose.yml](computer:///mnt/user-data/outputs/docker-compose.yml)** - Multi-service orchestration
4. **[ci-test.yml](computer:///mnt/user-data/outputs/ci-test.yml)** - GitHub Actions CI pipeline
5. **[train-models.yml](computer:///mnt/user-data/outputs/train-models.yml)** - GitHub Actions training automation

### ðŸ”œ **Still Need to Create**

6. Terraform AWS infrastructure
7. Airflow DAGs for batch experiments
8. MLflow integration patches
9. DVC setup
10. pytest test suite
11. Documentation files

---

## ðŸŽ¯ **Your CV Line (After Completion)**

```
Physics-Informed Neural Networks for Closed-Loop Glucose Control, Imperial College London
â€¢ Used GitHub Actions, MLflow, Docker Compose, AWS, Terraform and DVC to build a 
  production-grade CI/CD pipeline to automate testing, training, and deployment of 
  Physics-Informed Neural Networks for Type 1 Diabetes parameter estimation
â€¢ Built automated experiment orchestration pipeline using Airflow to systematically 
  evaluate 3 model architectures (BI-RNN, PINN, Modified-MLP) across 25 patient 
  datasets with automated metrics tracking and visualization
â€¢ Achieved <5% parameter estimation error for insulin sensitivity using 3-stage 
  inverse training with proper variable freezing and physics-informed loss constraints
```

**Same technologies as your friend âœ… + Medical AI context âœ… = ðŸ”¥ðŸ”¥ðŸ”¥**

---

## ðŸ“‹ **Implementation Roadmap**

### **Week 1: Foundation (Days 1-7)**

#### **Day 1: Docker Setup**
```bash
# 1. Copy files to your project
cp Dockerfile /path/to/T1D_PINN_Project/
cp docker-compose.yml /path/to/T1D_PINN_Project/

# 2. Add .dockerignore
cat > .dockerignore << 'EOF'
__pycache__
*.pyc
*.pyo
*.pyd
.Python
*.so
*.egg
*.egg-info
dist
build
.git
.github
.vscode
.idea
*.md
results/
mlruns/
.dvc/
EOF

# 3. Build and test
docker-compose build
docker-compose run training python -c "import tensorflow as tf; import torch; print('OK')"
```

#### **Day 2: GitHub Actions**
```bash
# 1. Create workflows directory
mkdir -p .github/workflows/

# 2. Copy workflow files
cp ci-test.yml .github/workflows/
cp train-models.yml .github/workflows/

# 3. Create test directory structure
mkdir -p tests/{unit,integration}
touch tests/__init__.py
touch tests/conftest.py

# 4. Add basic tests (I'll create these next)

# 5. Push to GitHub and watch CI run!
git add .github/ tests/
git commit -m "Add CI/CD pipelines"
git push
```

#### **Day 3-4: MLflow Integration**
```bash
# 1. Start MLflow
docker-compose up -d mlflow

# 2. Modify training scripts to log to MLflow
# (I'll create integration patches)

# 3. Run test experiment
python scripts/train_inverse.py \
    --config configs/pinn_inverse.yaml \
    --patient 3

# 4. View in MLflow UI
open http://localhost:5000
```

#### **Day 5-7: AWS Setup**
```bash
# 1. Install Terraform
brew install terraform  # Mac
# or
sudo apt-get install terraform  # Linux

# 2. Initialize Terraform
cd terraform/
terraform init

# 3. Plan infrastructure
terraform plan

# 4. Deploy (I'll create terraform configs)
terraform apply
```

---

### **Week 2: Cloud Training (Days 8-14)**

#### **Day 8-10: Terraform Infrastructure**

**Files I'll create:**
- `terraform/main.tf` - EC2 instance, S3 bucket, IAM roles
- `terraform/variables.tf` - Configurable parameters
- `terraform/outputs.tf` - Instance IPs, S3 bucket names

**What you'll get:**
- EC2 instance with GPU (g4dn.xlarge) or CPU (t3.medium)
- S3 bucket for results storage
- IAM roles for EC2 to access S3
- Security groups for SSH + MLflow access

#### **Day 11-12: Deploy to AWS**
```bash
# 1. SSH into EC2
ssh -i your-key.pem ubuntu@<EC2-IP>

# 2. Install dependencies
./setup_scripts/install_dependencies.sh

# 3. Clone and setup project
git clone https://github.com/YOUR_USERNAME/T1D_PINN_Project.git
cd T1D_PINN_Project
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -e .

# 4. Run test experiment
python scripts/train_inverse.py \
    --config configs/pinn_inverse.yaml \
    --patient 3
```

#### **Day 13-14: Batch Experiments**

Run comprehensive evaluation:
```bash
# All models Ã— 10 patients = 30 experiments
python scripts/batch_experiments.py \
    --models birnn pinn modified_mlp \
    --patients 2 3 4 5 6 7 8 9 10 11 \
    --parallel 3  # Run 3 at a time

# Or trigger via GitHub Actions
# (Click "Actions" â†’ "Automated Training" â†’ "Run workflow")
```

---

### **Week 3: Orchestration (Days 15-21)**

#### **Day 15-17: Airflow Setup**

**DAG I'll create:**
```python
# airflow/dags/comprehensive_training.py

from airflow import DAG
from airflow.operators.docker_operator import DockerOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'harsh',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    't1d_comprehensive_training',
    default_args=default_args,
    description='Train all models on all patients',
    schedule_interval=None,  # Manual trigger
    start_date=datetime(2024, 1, 1),
    catchup=False,
)

models = ['birnn', 'pinn', 'modified_mlp']
patients = range(2, 12)  # Pat2-11

for model in models:
    for patient in patients:
        DockerOperator(
            task_id=f'train_{model}_pat{patient}',
            image='t1d-pinn:latest',
            command=f'python scripts/train_inverse.py --config configs/{model}_inverse.yaml --patient {patient}',
            docker_url='unix://var/run/docker.sock',
            network_mode='bridge',
            dag=dag,
        )
```

**Start Airflow:**
```bash
docker-compose up -d airflow-webserver airflow-scheduler

# Access UI
open http://localhost:8080
# Login: admin/admin
```

#### **Day 18-19: DVC Setup**
```bash
# 1. Initialize DVC
dvc init

# 2. Add remote (S3)
dvc remote add -d storage s3://t1d-pinn-data

# 3. Track data
dvc add data/synthetic/
dvc add data/processed/

# 4. Push to S3
dvc push

# 5. Commit DVC files
git add data/.gitignore data/synthetic.dvc data/processed.dvc .dvc/
git commit -m "Add DVC data tracking"
```

#### **Day 20-21: Testing & Documentation**

Create comprehensive tests (I'll provide):
- Unit tests for models
- Integration tests for training
- Data validation tests

---

### **Week 4: Results & Paper (Days 22-28)**

#### **Day 22-23: Run Final Experiments**
```bash
# Trigger comprehensive training via Airflow
# Let it run overnight (all 30 experiments)
```

#### **Day 24-25: Collect & Analyze**
```bash
# Download results from S3
aws s3 sync s3://t1d-pinn-results/ results/

# Generate comparison tables
python scripts/evaluate.py --batch "results/*/Pat*" --output final_results.csv

# Create visualizations
python scripts/visualise_batch.py --results-csv final_results.csv
```

#### **Day 26-28: Write Paper**
- Use results from MLflow
- Generate figures from visualization scripts
- Compile final tables

---

## ðŸŽ“ **Skills You'll Learn**

### **DevOps & Infrastructure**
- âœ… Docker containerization
- âœ… Docker Compose orchestration
- âœ… Terraform infrastructure as code
- âœ… AWS EC2 deployment
- âœ… S3 storage management

### **CI/CD & Automation**
- âœ… GitHub Actions workflows
- âœ… Automated testing (pytest)
- âœ… Code quality (black, flake8)
- âœ… Continuous integration

### **MLOps**
- âœ… MLflow experiment tracking
- âœ… Model registry
- âœ… DVC data versioning
- âœ… Airflow orchestration

### **Best Practices**
- âœ… Production-grade code structure
- âœ… Reproducible experiments
- âœ… Version control (code + data + infrastructure)
- âœ… Documentation

---

## ðŸ’° **Cost Breakdown**

### **AWS Costs (One-Time Setup)**
- EC2 g4dn.xlarge (GPU): ~$0.52/hour
- EC2 t3.medium (CPU): ~$0.04/hour
- S3 storage: ~$0.023/GB/month
- Data transfer: ~$0.09/GB

### **Comprehensive Experiment Run**
- 30 experiments Ã— 2 hours each = 60 hours
- Running 3 in parallel = 20 hours total
- Cost: 20 hours Ã— $0.52/hour = **~$10.40**

### **Total for Full Project**
- Setup + testing: ~$5
- Comprehensive experiments: ~$10
- Buffer for mistakes: ~$10
- **Total: ~$25-30**

**Cheaper than a textbook!** ðŸ“šðŸ’°

---

## ðŸ“š **Next Files I'll Create**

Let me know if you want me to continue with:

1. **Terraform configs** (AWS infrastructure)
2. **Airflow DAGs** (batch orchestration)
3. **MLflow patches** (experiment tracking)
4. **Test suite** (pytest tests)
5. **Setup scripts** (automated installation)
6. **Documentation** (detailed guides)

I can create these in batches to avoid overwhelming you!

---

## ðŸš€ **Ready to Start?**

**Your path:**
1. Download the 5 files I've created
2. Follow Week 1 roadmap (Docker + GitHub Actions)
3. I'll create remaining files as you progress

**Or:**
- I can create ALL remaining files now
- You implement at your own pace

**What works better for you?** ðŸŽ¯
